\section*{Introduction, problem formulation, and positioning}

Adaptive spectral/\(hp\) CFD concentrates resolution where physics demands it, which is why it is attractive for high-fidelity simulations on practical meshes.
The same mechanism that makes it powerful also makes it difficult to run efficiently at scale: adaptation changes the \emph{shape} of the workload over time.
Regions of the domain may carry significantly higher polynomial order \(P\) (and/or smaller element size \(h\)), and those localized regions can dominate timestep cost if they accumulate on the same rank or device.

At scale, end-to-end wall time is frequently set by the slowest partition.
This is not merely an implementation detail: globally coupled phases (in particular solver phases) introduce synchronization that forces ranks/devices to wait for one another.
Heterogeneous clusters amplify the effect further—mixed CPUs, SMT, and GPUs mean that “equal work” depends on device throughput and on which phases dominate the timestep.
Consequently, balancing by element count is not meaningful in adaptive high-order settings: equal counts do not imply equal time when per-element cost varies strongly with \(P\).

The decision problem addressed here is therefore operational and model-based:
given an adaptive simulation whose \(P\)-distribution evolves, \emph{when} should a repartition/migration be performed, and \emph{how much} data should be moved, to reduce wall time without creating churn?
A periodic policy (“rebalance every \(k\)”) is simple but can pay overhead even when the mapping changes weakly, including no-op or near-no-op events.
A triggered policy can be better, but only if it can predict net gain reliably enough to act as a gate and if it controls migration so that rebalancing does not become the dominant cost.

We evaluate policies using phase-aware wall-time metrics and explicit overhead accounting: wall-time per step \(t_{\text{step}}\), imbalance ratio \(\rho=\max/\text{mean}\), effective efficiency \(\eta\approx \text{mean}/\max\), event counts (including no-ops), and migration intensity (elements moved and migration time).
A policy is “better” only if it reduces end-to-end wall time while keeping overhead bounded and preserving solution acceptability (QoI tolerance).

This proposal is grounded in the spectral element and spectral/\(hp\) CFD foundations \cite{Patera1984SpectralElement,KarniadakisSherwin2005}, and it is anchored in Nektar++ as a credible target platform for publishable algorithm work \cite{Cantwell2015NektarPP,Cantwell2020NektarPP}.
On the load-balancing side, partitioning and dynamic rebalancing baselines are well-established in HPC (e.g., multilevel graph partitioning \cite{KarypisKumar1998METIS} and dynamic services/frameworks \cite{Devine2002Zoltan}), and scalable adaptive-mesh infrastructures motivate what “practical overhead” looks like in real systems \cite{Burstedde2011p4est}.
The gap we focus on is the intersection of (i) superlinear cost growth with \(P\), (ii) explicit modeling of rebalance overhead, and (iii) heterogeneity-aware decision rules that avoid wasted events and migration churn.

The remainder of this proposal makes that gap precise: it specifies a model for predicted benefit versus cost, defines a trigger rule with bounded migration, and lays out an evaluation plan designed to produce phase-aware, publishable performance evidence.
